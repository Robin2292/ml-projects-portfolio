{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Observations</u>: \n",
    "\n",
    "1) closest distance to line\n",
    "2) heading angle relative to line\n",
    "3) road curvature at closest point\n",
    "4) vx sensor\n",
    "5) heading angular rate gyro\n",
    "6) look ahead road curvatures\n",
    "\n",
    "<u>Reward Components</u>:\n",
    "\n",
    "1) road progress reward\n",
    "2) bounded piecewise quadratic road proximity reward (positive reward conditional on road progress)\n",
    "3) bounded piecewise quadratic-linear speed reward with adaptive speed limit based on road curvature multiplier (positive reward conditional on road progress)\n",
    "4) bounded piecewise heading angle reward\n",
    "5) angular velocity smoothness penalty\n",
    "\n",
    "<u>Model</u>:\n",
    "\n",
    "SAC + Modified Neural Net:\n",
    "\n",
    "policy_kwargs = dict(net_arch=[256, 256, 256])\n",
    "\n",
    "`road_randomization_params = {\n",
    "    'num_elements_range': (3, 10),             # Highly variable number of road elements\n",
    "    'straight_length_range': (50.0, 300.0),    # Straight segments can be very short or long\n",
    "    'curvature_range': (-1/50.0, 1/50.0),      # Very tight curves possible\n",
    "    'angle_range': (10.0, 150.0)                 # Wide range of angles for curves   \n",
    "}`\n",
    "\n",
    "\n",
    "* High-speed road with speed limit 100 km/h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ai4rgym\n",
    "from ai4rgym.envs.road import Road\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from stable_baselines3 import PPO, SAC, DDPG\n",
    "from utils import ensure_dir, ensure_dirs, eval_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFY THE VEHCILE PARAMETERS\n",
    "bicycle_model_parameters = {\n",
    "    \"Lf\" : 0.55*2.875,\n",
    "    \"Lr\" : 0.45*2.875,\n",
    "    \"m\"  : 2000.0,\n",
    "    \"Iz\" : (1.0/12.0) * 2000.0 * (4.692**2+1.850**2),\n",
    "    \"Cm\" : (1.0/100.0) * (1.0 * 400.0 * 9.0) / 0.2286,\n",
    "    \"Cd\" : 0.5 * 0.24 * 2.2204 * 1.202,\n",
    "    \"delta_offset\" : 0 * np.pi/180,\n",
    "    \"delta_request_max\" : 45 * np.pi/180,\n",
    "    \"Ddelta_lower_limit\" : -45 * np.pi/180,\n",
    "    \"Ddelta_upper_limit\" :  45 * np.pi/180,\n",
    "    \"v_transition_min\": 3.0,    # v_transition_min = 3.0 m/s\n",
    "    \"v_transition_max\": 5.0,    # v_transition_max = 5.0 m/s\n",
    "    \"body_len_f\" : (0.55*2.875) * 1.5,\n",
    "    \"body_len_r\" : (0.45*2.875) * 1.5,\n",
    "    \"body_width\" : 2.50,\n",
    "}\n",
    "\n",
    "# SPECIFY THE ROAD\n",
    "\n",
    "# This road will not be used for training / evaluation\n",
    "# this is just kept in place to initialize the environment\n",
    "road_elements_list = [\n",
    "    {\"type\":\"straight\", \"length\":100.0},\n",
    "    {\"type\":\"curved\", \"curvature\":1/800.0, \"angle_in_degrees\":15.0},\n",
    "    {\"type\":\"straight\", \"length\":100.0},\n",
    "    {\"type\":\"curved\", \"curvature\":-1/400.0, \"angle_in_degrees\":30.0},\n",
    "    {\"type\":\"straight\", \"length\":100.0},\n",
    "]\n",
    "\n",
    "# SPECIFY THE NUMERICAL INTEGRATION DETAILS\n",
    "numerical_integration_parameters = {\n",
    "    \"method\" : \"rk4\",\n",
    "    \"Ts\" : 0.05,\n",
    "    \"num_steps_per_Ts\" : 1,\n",
    "}\n",
    "\n",
    "# SPECIFY THE INITIAL STATE DISTRIBUTION\n",
    "\n",
    "py_init_min = -1.0\n",
    "py_init_max =  1.0\n",
    "\n",
    "v_init_min_in_kmh = 55.0\n",
    "v_init_max_in_kmh = 65.0\n",
    "\n",
    "py_init_min = -1.0\n",
    "py_init_max =  1.0\n",
    "\n",
    "v_init_min_in_kmh = 55.0\n",
    "v_init_max_in_kmh = 65.0\n",
    "\n",
    "initial_state_bounds = {\n",
    "    \"px_init_min\" : 0.0,\n",
    "    \"px_init_max\" : 0.0,\n",
    "    \"py_init_min\" : py_init_min,\n",
    "    \"py_init_max\" : py_init_max,\n",
    "    \"theta_init_min\" : 0.0,\n",
    "    \"theta_init_max\" : 0.0,\n",
    "    \"vx_init_min\" : v_init_min_in_kmh * (1.0/3.6),\n",
    "    \"vx_init_max\" : v_init_max_in_kmh * (1.0/3.6),\n",
    "    \"vy_init_min\" : 0.0,\n",
    "    \"vy_init_max\" : 0.0,\n",
    "    \"omega_init_min\" : 0.0,\n",
    "    \"omega_init_max\" : 0.0,\n",
    "    \"delta_init_min\" : 0.0,\n",
    "    \"delta_init_max\" : 0.0,\n",
    "}\n",
    "\n",
    "# SPECIFY THE TERMINATION PARAMETERS\n",
    "termination_parameters = {\n",
    "    \"speed_lower_bound\"  :  (20.0/3.6),            # to maintain speed above 20 km/h \n",
    "    \"speed_upper_bound\"  :  (200.0/3.6),\n",
    "    \"distance_to_closest_point_upper_bound\"  : 20.0,\n",
    "    \"reward_speed_lower_bound\"  :  -5000.0,\n",
    "    \"reward_speed_upper_bound\"  :  -2000.0,\n",
    "    \"reward_distance_to_closest_point_upper_bound\"  :  -5000.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Observations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFY THE OBSERVATION PARAMETERS\n",
    "observation_parameters = {\n",
    "    \"should_include_ground_truth_px\"                       :  \"info\",\n",
    "    \"should_include_ground_truth_py\"                       :  \"info\",\n",
    "    \"should_include_ground_truth_theta\"                    :  \"info\",\n",
    "    \"should_include_ground_truth_vx\"                       :  \"info\",\n",
    "    \"should_include_ground_truth_vy\"                       :  \"info\",\n",
    "    \"should_include_ground_truth_omega\"                    :  \"info\",\n",
    "    \"should_include_ground_truth_delta\"                    :  \"info\",\n",
    "    \"should_include_road_progress_at_closest_point\"        :  \"info\",\n",
    "    \"should_include_vx_sensor\"                             :  \"obs\",\n",
    "    \"should_include_distance_to_closest_point\"             :  \"obs\",\n",
    "    \"should_include_heading_angle_relative_to_line\"        :  \"obs\",\n",
    "    \"should_include_heading_angular_rate_gyro\"             :  \"obs\",\n",
    "    \"should_include_closest_point_coords_in_body_frame\"    :  \"info\",\n",
    "    \"should_include_look_ahead_line_coords_in_body_frame\"  :  \"info\",\n",
    "    \"should_include_road_curvature_at_closest_point\"       :  \"obs\",\n",
    "    \"should_include_look_ahead_road_curvatures\"            :  \"obs\",\n",
    "\n",
    "    \"scaling_for_ground_truth_px\"                       :  1.0,\n",
    "    \"scaling_for_ground_truth_py\"                       :  1.0,\n",
    "    \"scaling_for_ground_truth_theta\"                    :  1.0,\n",
    "    \"scaling_for_ground_truth_vx\"                       :  1.0,\n",
    "    \"scaling_for_ground_truth_vy\"                       :  1.0,\n",
    "    \"scaling_for_ground_truth_omega\"                    :  1.0,\n",
    "    \"scaling_for_ground_truth_delta\"                    :  1.0,\n",
    "    \"scaling_for_road_progress_at_closest_point\"        :  1.0,\n",
    "    \"scaling_for_vx_sensor\"                             :  1.0,\n",
    "    \"scaling_for_distance_to_closest_point\"             :  1.0,\n",
    "    \"scaling_for_heading_angle_relative_to_line\"        :  1.0,\n",
    "    \"scaling_for_heading_angular_rate_gyro\"             :  1.0,\n",
    "    \"scaling_for_closest_point_coords_in_body_frame\"    :  1.0,\n",
    "    \"scaling_for_look_ahead_line_coords_in_body_frame\"  :  1.0,\n",
    "    \"scaling_for_road_curvature_at_closest_point\"       :  1.0,\n",
    "    \"scaling_for_look_ahead_road_curvatures\"            :  1.0,\n",
    "\n",
    "    \"vx_sensor_bias\"    : 0.0,\n",
    "    \"vx_sensor_stddev\"  : 0.1,\n",
    "\n",
    "    \"distance_to_closest_point_bias\"    :  0.0,\n",
    "    \"distance_to_closest_point_stddev\"  :  0.01,\n",
    "\n",
    "    \"heading_angle_relative_to_line_bias\"    :  0.0,\n",
    "    \"heading_angle_relative_to_line_stddev\"  :  0.01,\n",
    "\n",
    "    \"heading_angular_rate_gyro_bias\"    :  0.0,\n",
    "    \"heading_angular_rate_gyro_stddev\"  :  0.01,\n",
    "\n",
    "    \"closest_point_coords_in_body_frame_bias\"    :  0.0,\n",
    "    \"closest_point_coords_in_body_frame_stddev\"  :  0.0,\n",
    "\n",
    "    \"look_ahead_line_coords_in_body_frame_bias\"    :  0.0,\n",
    "    \"look_ahead_line_coords_in_body_frame_stddev\"  :  0.0,\n",
    "\n",
    "    \"road_curvature_at_closest_point_bias\"    :  0.0,\n",
    "    \"road_curvature_at_closest_point_stddev\"  :  0.0,\n",
    "\n",
    "    \"look_ahead_road_curvatures_bias\"    :  0.0,\n",
    "    \"look_ahead_road_curvatures_stddev\"  :  0.0,\n",
    "\n",
    "    \"look_ahead_line_coords_in_body_frame_distance\"    :  100.0,\n",
    "    \"look_ahead_line_coords_in_body_frame_num_points\"  :  10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class RewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(RewardWrapper, self).__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # Create your own reward here\n",
    "        reward = reward\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\"\"\"\n",
    "\n",
    "class RewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(RewardWrapper, self).__init__(env)\n",
    "        self.previous_progress_at_closest_point = None\n",
    "        self.previous_heading_angular_rate_gyro = None\n",
    "        self.previous_heading_angle = None\n",
    "\n",
    "        # set speed limit\n",
    "        SPEED_LIMIT = 100.0 / 3.6 # 100 km/h\n",
    "        self.speed_limit = SPEED_LIMIT \n",
    "        \n",
    "        # constants chosen so that the maximum reward obtained from road proximity and speed are the same order of magnitude (i.e. normalized)\n",
    "        self.progress_reward_constant = 2.0 # 1.0 \n",
    "        self.road_proximity_reward_constant_1 = 1.0 \n",
    "        self.road_proximity_reward_constant_2 = 2.0 \n",
    "        self.speed_reward_constant = 10.0 # 4.0 \n",
    "        self.heading_reward_constant_1 = 2.0    \n",
    "        self.heading_reward_constant_2 = 1.0 # 0.16\n",
    "        self.angular_velocity_smoothness_reward_constant = 0.20 \n",
    "        self.high_road_curvature_threshold = 1/250.0 \n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        ###############################################    \n",
    "        # Custom reward, overwrite the default reward\n",
    "        ###############################################    \n",
    "        reward = 0.0 # overwrite default reward\n",
    "\n",
    "        # set progress reward\n",
    "        progress_at_closest_point = info[\"road_progress_at_closest_point\"]\n",
    "        if self.previous_progress_at_closest_point is not None:\n",
    "            progress_reward = (progress_at_closest_point - self.previous_progress_at_closest_point) * self.progress_reward_constant\n",
    "        else:\n",
    "            progress_reward = 0.0\n",
    "        self.previous_progress_at_closest_point = progress_at_closest_point\n",
    "\n",
    "\n",
    "        # give higher progress reward for progress closer to the end of the road, to encourage reaching the destination faster\n",
    "        # also don't give positive road proximity reward if the vehicle is not progressing\n",
    "\n",
    "        # set road proximity reward, i.e. how close the vehicle is to the road\n",
    "        #  positive reward if close to the road i.e. within 2m\n",
    "        #  negative reward if far from the road i.e. more than 2m\n",
    "        d = abs(observation[\"distance_to_closest_point\"])\n",
    "        \n",
    "        road_proximity_reward = 0.0\n",
    "        if d < 0.5:\n",
    "            if progress_reward > 0.0:\n",
    "                road_proximity_reward = 3.5 * self.road_proximity_reward_constant_1 * (1.0 - d**2)\n",
    "\n",
    "        elif 0.5 <= d < 2.0:\n",
    "            if progress_reward > 0.0:\n",
    "                road_proximity_reward = self.road_proximity_reward_constant_1 * (2.0 - d)**2\n",
    "        else:\n",
    "            road_proximity_reward = -self.road_proximity_reward_constant_2 * np.tanh((d - 2.0)**4)\n",
    "\n",
    "\n",
    "        # set speed reward \n",
    "        # positive reward if speed is less than speed limit, otherwise negative reward\n",
    "        # positive reward also reduced in proportion to max look ahead road curvature in the next 3 points \n",
    "        vx = abs(observation[\"vx_sensor\"])\n",
    "        #road_curvature_closest_point = max(np.abs(observation[\"look_ahead_road_curvatures\"][:3]))   \n",
    "        road_curvature_closest_point = abs(observation[\"road_curvature_at_closest_point\"]) \n",
    "        road_curvature_closest_point = max(road_curvature_closest_point, np.max(np.abs(observation[\"look_ahead_road_curvatures\"])))\n",
    "\n",
    "        # road curvature multiplier to encourage slower speeds on high curvature roads\n",
    "        if road_curvature_closest_point >= self.high_road_curvature_threshold:\n",
    "            road_curvature_multiplier = 1.0 /  (1.0 + (400.0*(road_curvature_closest_point - self.high_road_curvature_threshold)**(1.2)))\n",
    "        else:\n",
    "            road_curvature_multiplier = 1.0    \n",
    "\n",
    "        # adaptive speed limit based on road curvature, lower speed limit for higher curvature roads\n",
    "        adjusted_speed_limit = self.speed_limit * road_curvature_multiplier\n",
    "\n",
    "        # piecewise linear below speed limit (to encourage driving at a speed closer to the adjusted speed limit)\n",
    "        if vx < 0.9*adjusted_speed_limit:\n",
    "            speed_reward =  self.speed_reward_constant * (vx / (0.9*adjusted_speed_limit))**2 \n",
    "\n",
    "        elif 0.9*adjusted_speed_limit <= vx < adjusted_speed_limit:\n",
    "            #speed_reward =  -self.speed_reward_constant * (vx - adjusted_speed_limit) / (0.1*adjusted_speed_limit)\n",
    "            speed_reward =  - 10.0 * self.speed_reward_constant * (vx - adjusted_speed_limit) / (0.1*adjusted_speed_limit)\n",
    "\n",
    "        else:\n",
    "            #speed_reward = -self.speed_reward_constant * np.tanh((vx - adjusted_speed_limit)**4)\n",
    "            #speed_reward = self.speed_reward_constant * ( (1.0/((vx - adjusted_speed_limit+ 1)**4)) - 1.0)\n",
    "            speed_reward = 0.5 *self.speed_reward_constant * ( (1.0/((vx - adjusted_speed_limit+ 1)**4)) - 1.0)\n",
    "\n",
    "\n",
    "        # set reward for maintaining small heading angle relative to the line (negative reward proportional to the heading angle)\n",
    "        # for heading angle smaller than 2 degrees, give positive reward, otherwise give negative reward\n",
    "        heading_angle_degrees = abs(observation[\"heading_angle_relative_to_line\"]) * 180.0 / np.pi \n",
    "        heading_reward = 0.0\n",
    "        if heading_angle_degrees < 1.0:\n",
    "            # give positive heading reward only if there is progress\n",
    "            if progress_reward > 0.0: \n",
    "                heading_reward = self.heading_reward_constant_1 * (heading_angle_degrees-1.0)**2 / 4.0\n",
    "        else:\n",
    "            heading_reward = -self.heading_reward_constant_2 * abs(heading_angle_degrees)\n",
    "\n",
    "        # set reward for encouraging smoother steering, by penalize large gradients in heading angle and angular velocity\n",
    "        if self.previous_heading_angle is not None:\n",
    "            heading_angle = abs(observation[\"heading_angle_relative_to_line\"])\n",
    "            heading_angle_change_degrees = abs(heading_angle - self.previous_heading_angle) * 180.0 / np.pi\n",
    "            angular_velocity_smoothness_reward = -self.angular_velocity_smoothness_reward_constant * heading_angle_change_degrees\n",
    "        else:\n",
    "            angular_velocity_smoothness_reward = 0.0\n",
    "         \n",
    "\n",
    "        # TODO: check termination condition and set termination rewards\n",
    "        terminated = False\n",
    "        termination_reward = 0.0\n",
    "        if (progress_at_closest_point >= self.env.unwrapped.total_road_length_for_termination):\n",
    "            #print(\"(prog,tot_len) = ( \" + str(info_dict[\"progress_at_closest_p\"]) + \" , \" + str(self.total_road_length) + \" )\" )\n",
    "            terminated = True\n",
    "        # > Terminate for being outside of the speed range\n",
    "        if (vx > self.env.unwrapped.termination_speed_upper_bound):\n",
    "            terminated = True\n",
    "            termination_reward += self.env.unwrapped.termination_reward_for_speed_upper_bound\n",
    "        if (observation[\"vx_sensor\"] < self.env.unwrapped.termination_speed_lower_bound):\n",
    "            terminated = True\n",
    "            termination_reward += self.env.unwrapped.termination_reward_for_speed_lower_bound\n",
    "        # > Terminate for deviating too much from the line\n",
    "        if (d > self.env.unwrapped.termination_distance_to_closest_point_upper_bound):\n",
    "            terminated = True\n",
    "            termination_reward += self.env.unwrapped.termination_reward_for_distance_to_closest_point_upper_bound\n",
    "\n",
    "        # no truncation\n",
    "        truncated = False\n",
    "\n",
    "        # add up all the reward contributions (ignore speed reward for now)\n",
    "        reward = progress_reward + road_proximity_reward + speed_reward + heading_reward + angular_velocity_smoothness_reward + termination_reward\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_env(road_elements_list):\n",
    "    env = gym.make(\n",
    "        \"ai4rgym/autonomous_driving_env\",\n",
    "        render_mode=None,\n",
    "        bicycle_model_parameters=bicycle_model_parameters,\n",
    "        road_elements_list=road_elements_list,\n",
    "        numerical_integration_parameters=numerical_integration_parameters,\n",
    "        termination_parameters=termination_parameters,\n",
    "        initial_state_bounds=initial_state_bounds,\n",
    "        observation_parameters=observation_parameters,\n",
    "    )\n",
    "\n",
    "    # Set the integration method and time step\n",
    "    Ts_sim = 0.05\n",
    "    integration_method = \"rk4\"\n",
    "    env.unwrapped.set_integration_method(integration_method)\n",
    "    env.unwrapped.set_integration_Ts(Ts_sim)\n",
    "\n",
    "    # Set the road condition\n",
    "    env.unwrapped.set_road_condition(road_condition=\"wet\")\n",
    "\n",
    "    # Rescale actions and wrap the environment\n",
    "    env = gym.wrappers.RescaleAction(env, min_action=-1, max_action=1)\n",
    "    env = RewardWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Randomization Wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainRandomizationWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, road_randomization_params=None):\n",
    "        super(DomainRandomizationWrapper, self).__init__(env)\n",
    "        self.road_randomization_params = road_randomization_params\n",
    "\n",
    "    def generate_random_road_elements_list(self):\n",
    "        params = self.road_randomization_params or {}\n",
    "        num_elements_range = params.get('num_elements_range', (2, 5))\n",
    "        straight_length_range = params.get('straight_length_range', (50.0, 200.0))\n",
    "        curvature_range = params.get('curvature_range', (-1/500.0, 1/500.0))\n",
    "        angle_range = params.get('angle_range', (10.0, 60.0))\n",
    "\n",
    "        road_elements = []\n",
    "        num_elements = random.randint(*num_elements_range)\n",
    "\n",
    "        for _ in range(num_elements):\n",
    "            element_type = random.choice(['straight', 'curved'])\n",
    "            if element_type == 'straight':\n",
    "                length = random.uniform(*straight_length_range)\n",
    "                road_elements.append({\"type\": \"straight\", \"length\": length})\n",
    "            else:\n",
    "                curvature = random.uniform(*curvature_range)\n",
    "                angle = random.uniform(*angle_range)\n",
    "                road_elements.append({\n",
    "                    \"type\": \"curved\",\n",
    "                    \"curvature\": curvature,\n",
    "                    \"angle_in_degrees\": angle\n",
    "                })\n",
    "        return road_elements\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        # Generate a new random road\n",
    "        self.unwrapped.road_elements_list = self.generate_random_road_elements_list()\n",
    "        self.unwrapped.road = Road(epsilon_c=(1/10000), road_elements_list=self.unwrapped.road_elements_list)\n",
    "        self.unwrapped.total_road_length = self.unwrapped.road.get_total_length()\n",
    "        self.unwrapped.total_road_length_for_termination = max(self.unwrapped.total_road_length - 0.1, 0.9999 * self.unwrapped.total_road_length)\n",
    "        # Call the original reset method\n",
    "        return self.env.reset(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Random Roads: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Standard Randomization\n",
    "# road_randomization_params = {\n",
    "#      'num_elements_range': (3, 6),          # Road will have 3 to 6 elements\n",
    "#      'straight_length_range': (100.0, 150.0), # Length of straight segments between 100 and 150 meters\n",
    "#      'curvature_range': (-1/300.0, 1/300.0), # Curvature between -1/300 and 1/300 for curves\n",
    "#      'angle_range': (20.0, 45.0)            # Angle of the curves between 20 and 45 degrees\n",
    "#  }\n",
    "\n",
    "# # Example 2: More Straight Roads\n",
    "# road_randomization_params = {\n",
    "#     'num_elements_range': (2, 4),            # Fewer elements, 2 to 4\n",
    "#     'straight_length_range': (150.0, 300.0), # Longer straight segments between 150 and 300 meters\n",
    "#     'curvature_range': (-1/1000.0, 1/1000.0), # Small curvature for gentler curves\n",
    "#     'angle_range': (10.0, 30.0)              # Curves have lower angles between 10 and 30 degrees\n",
    "# }\n",
    "\n",
    "# # Example 3: More challenging roads with Tighter curves\n",
    "# road_randomization_params = {\n",
    "#       'num_elements_range': (3, 7),            \n",
    "#       'straight_length_range': (50.0, 150.0), \n",
    "#       'curvature_range': (-1/100.0, 1/100.0),  \n",
    "#       'angle_range': (5.0, 60.0)        \n",
    "# }\n",
    "\n",
    "road_randomization_params = {\n",
    "    'num_elements_range': (3, 10),             # Highly variable number of road elements\n",
    "    'straight_length_range': (50.0, 300.0),    # Straight segments can be very short or long\n",
    "    'curvature_range': (-1/50.0, 1/50.0),      # Very tight curves possible\n",
    "    'angle_range': (10.0, 150.0)                 # Wide range of angles for curves   \n",
    "}\n",
    "\n",
    "\n",
    "# # Example 4: Completely Randomized\n",
    "# road_randomization_params = {\n",
    "#     'num_elements_range': (1, 10),             # Highly variable number of road elements\n",
    "#     'straight_length_range': (50.0, 300.0),    # Straight segments can be very short or long\n",
    "#     'curvature_range': (-1/50.0, 1/50.0),      # Very tight curves possible\n",
    "#     'angle_range': (10.0, 90.0)                # Wide range of angles for curves\n",
    "# }\n",
    "\n",
    "random_env = create_env(road_elements_list = road_elements_list, )\n",
    "random_env = DomainRandomizationWrapper(random_env, road_randomization_params=road_randomization_params)\n",
    "\n",
    "# Reset the random env - this will create a new random road\n",
    "random_env.reset()\n",
    "\n",
    "# Render the road\n",
    "random_env.render_matplotlib_init_figure()\n",
    "random_env.render_matplotlib_plot_road()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Training/Evaluation Environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ------------- MAKE CHANGES TO THE ROAD RANDOMIZATION PARAMS HERE ------- ###\n",
    "#road_randomization_params = None\n",
    "road_randomization_params = {\n",
    "    'num_elements_range': (3, 10),             # Highly variable number of road elements\n",
    "    'straight_length_range': (50.0, 300.0),    # Straight segments can be very short or long\n",
    "    'curvature_range': (-1/50.0, 1/50.0),      # Very tight curves possible\n",
    "    'angle_range': (10.0, 150.0)                 # Wide range of angles for curves   \n",
    "}\n",
    "\n",
    "### ------------------------------------------------------------------------ ###\n",
    "\n",
    "# Create the training environment with domain randomization\n",
    "train_env = create_env(road_elements_list=road_elements_list)  # Road elements will be randomized\n",
    "train_env = DomainRandomizationWrapper(train_env, road_randomization_params = road_randomization_params)\n",
    "train_env = RewardWrapper(train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fixed, difficult road for evaluation\n",
    "road_elements_list_difficult = [\n",
    "    {\"type\": \"straight\", \"length\": 150.0},\n",
    "    {\"type\": \"curved\", \"curvature\": 1/100.0, \"angle_in_degrees\": 110.0},\n",
    "    {\"type\": \"straight\", \"length\": 50.0},\n",
    "    {\"type\": \"curved\", \"curvature\": -1/70.0, \"angle_in_degrees\": 150.0},\n",
    "    {\"type\": \"straight\", \"length\": 50.0},\n",
    "    {\"type\": \"curved\", \"curvature\": 1/50.0, \"angle_in_degrees\": 160.0},\n",
    "    {\"type\": \"straight\", \"length\": 100.0},\n",
    "    {\"type\": \"curved\", \"curvature\": -1/200.0, \"angle_in_degrees\": 60.0},\n",
    "     \n",
    "]\n",
    "\n",
    "# Create the evaluation environment with the fixed difficult road\n",
    "eval_env = create_env(road_elements_list_difficult)\n",
    "\n",
    "# Render the road\n",
    "eval_env.render_matplotlib_init_figure()\n",
    "eval_env.render_matplotlib_plot_road()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition and Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, SAC, DDPG\n",
    "\n",
    "model_name = \"SAC_dr_wet_long_6\"\n",
    "\n",
    "TIMESTEPS_PER_EPOCH = 50000\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "logdir = \"logs\"\n",
    "models_dir = f\"models/{model_name}\"\n",
    "figs_dir_train = f\"models/{model_name}/figs_train\"\n",
    "figs_dir_test = f\"models/{model_name}/figs_test\"\n",
    "\n",
    "ensure_dirs([logdir, models_dir, figs_dir_train, figs_dir_test])\n",
    "\n",
    "policy_kwargs = dict(net_arch=[256, 256, 256])\n",
    "\n",
    "# double batch size, double replay buffer size (important for off-policy models), deeper neural network\n",
    "model = SAC(\"MultiInputPolicy\", train_env, policy_kwargs=policy_kwargs, verbose = 1, tensorboard_log=logdir, batch_size=512, buffer_size=2000000)\n",
    "\n",
    "def train(start_epoch=1, num_epochs=10, timesteps_per_epoch=50000):\n",
    "    for i in range(start_epoch, start_epoch+num_epochs):\n",
    "        model.learn(\n",
    "            total_timesteps=timesteps_per_epoch,\n",
    "            reset_num_timesteps=False,\n",
    "            tb_log_name=f\"{model_name}\"\n",
    "        )\n",
    "        model.save(f\"{models_dir}/{timesteps_per_epoch * i}\")\n",
    "        # Evaluate on the most recent training road\n",
    "        eval_model(train_env, model, figs_dir_train, timesteps_per_epoch * i)\n",
    "        # Evaluate on evaluation road\n",
    "        eval_model(eval_env, model, figs_dir_test, timesteps_per_epoch * i)\n",
    "\n",
    "    start_epoch += num_epochs    \n",
    "    return start_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 1\n",
    "num_epochs = 50\n",
    "timesteps_per_epoch = 2000\n",
    "\n",
    "start_epoch = train(start_epoch, num_epochs, timesteps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the RL Policy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load an RL model from the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SAC_dr_wet_long_6\"\n",
    "model_idx = 100000\n",
    "\n",
    "path_for_saving_figures = f\"models/{model_name}/eval\"\n",
    "ensure_dir(path_for_saving_figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the RL model into a policy class with a 'standardize' interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from policies.rl_policy import RLPolicy\n",
    "\n",
    "# Put the RL model into a policy class\n",
    "rl_policy = RLPolicy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform One simulation of the policy, plot the time series results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function for simulating the autonomous driving environment\n",
    "from evaluation.evaluation_for_autonomous_driving import simulate_policy\n",
    "from evaluation.evaluation_for_autonomous_driving import plot_results_from_time_series_dict\n",
    "\n",
    "# Specify the length of the simulation in time steps\n",
    "# > If termination or truncation is flagged by the \"step\" function,\n",
    "#   Then, the simulation ends.\n",
    "N_sim = 30000\n",
    "\n",
    "# Specify the seed for when the simulate function resets the random number generator\n",
    "sim_seed = 1;\n",
    "\n",
    "# Call the function for simulating a given RL model\n",
    "sim_time_series_dict = simulate_policy(eval_env, N_sim, rl_policy, seed=sim_seed, verbose=1)\n",
    "\n",
    "# Call the plotting function\n",
    "file_name_suffix = \"example\"\n",
    "plot_details_list = plot_results_from_time_series_dict(eval_env, sim_time_series_dict, path_for_saving_figures, file_name_suffix, should_plot_reward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animate the time series results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def animate_from_sim_time_series_dict(sim_time_series_dict, Ts, path_for_saving_figures):\n",
    "    # Extract the necessary trajectory information from the \"sim_time_series_dict\"\n",
    "    px_traj    = sim_time_series_dict[\"px\"]\n",
    "    py_traj    = sim_time_series_dict[\"py\"]\n",
    "    theta_traj = sim_time_series_dict[\"theta\"]\n",
    "    delta_traj = sim_time_series_dict[\"delta\"]\n",
    "    # Call the environments function to create the simulation\n",
    "    ani = eval_env.unwrapped.render_matplotlib_animation_of_trajectory(px_traj, py_traj, theta_traj, delta_traj, Ts, traj_increment=3)\n",
    "    # Save the animation\n",
    "    ani.save(f\"{path_for_saving_figures}/trajectory_animation.gif\")\n",
    "    print(f'Saved animation at {path_for_saving_figures}/trajectory_animation.gif')\n",
    "    # Return the animation object\n",
    "    return ani\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Call the animation functoin\n",
    "ani = animate_from_sim_time_series_dict(sim_time_series_dict, numerical_integration_parameters[\"Ts\"], path_for_saving_figures)\n",
    "# Display the animation\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a performance Metrics Function per simulation time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance_metrics_from_time_series(sim_time_series_dict):\n",
    "    # Compute the statistics of the distance to the line\n",
    "    # > Note that this is signed value, hence need to take absolute value of the data.\n",
    "    abs_dist_to_line_time_series = np.abs(sim_time_series_dict[\"distance_to_closest_point\"])\n",
    "    avg_dist  = np.nanmean(abs_dist_to_line_time_series)\n",
    "    std_dist  = np.nanstd(abs_dist_to_line_time_series)\n",
    "    max_dist  = np.nanmax(abs_dist_to_line_time_series)\n",
    "\n",
    "    # Compute the statistics of the speed in the forward direction (i.e., the body-frame x-axis direction)\n",
    "    speed_time_series = np.abs(sim_time_series_dict[\"vx\"])\n",
    "    avg_speed = np.nanmean(speed_time_series)\n",
    "    std_speed = np.nanstd(speed_time_series)\n",
    "    max_speed = np.nanmax(speed_time_series)\n",
    "    min_speed = np.nanmin(speed_time_series)\n",
    "\n",
    "    # Return the results\n",
    "    return {\n",
    "        \"avg_dist\"   :  avg_dist,\n",
    "        \"std_dist\"   :  std_dist,\n",
    "        \"max_dist\"   :  max_dist,\n",
    "        \"avg_speed\"  :  avg_speed * 3.6,\n",
    "        \"std_speed\"  :  std_speed * 3.6,\n",
    "        \"max_speed\"  :  max_speed * 3.6,\n",
    "        \"min_speed\"  :  min_speed * 3.6,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Performance Metric Function for one simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate with the same seed as above to check that it gets the results consistent\n",
    "sim_seed = 1;\n",
    "sim_time_series_dict = simulate_policy(eval_env, N_sim, rl_policy, seed=sim_seed, verbose=1)\n",
    "\n",
    "# Call the function for computing the performance metrics\n",
    "pm_dict = compute_performance_metrics_from_time_series(sim_time_series_dict)\n",
    "\n",
    "# Display the performance metric values\n",
    "print(\"Performance Metric dictionary:\")\n",
    "print(pm_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:**\n",
    "> Please refer to the notebook `autonomous_driving_gym_v2024_08_24.ipynb` for more details on evaluation and performance metrics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
